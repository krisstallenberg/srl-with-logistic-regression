{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd4652c-0560-4a97-bd12-03f8891b671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pprint  # Add this line\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3699ebb-8b93-4442-856c-0768c1007c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path.replace('.conllu', '.preprocessed.pkl'), 'rb') as pickle_file:\n",
    "        return pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2a59ef-8309-4845-b4a4-9ed487057a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_model(data):\n",
    "    \"\"\"Convert feature dict to pandas DataFrame to handle data for training.\n",
    "    Return a pandas df with the relevant features for training the model.\n",
    "\n",
    "    Paramenters:\n",
    "    -data: a list of objects, where each object represents one 'frame' in a sentence.\n",
    "    \"\"\"\n",
    "    list_features = [] #creating an empty list where the data will be stored\n",
    "    \n",
    "    for sentences in data:\n",
    "        for token_dict in sentences:\n",
    "            dict_feat = token_dict['features'] #grabbing the nested dictionaries where the features are stored\n",
    "            list_features.append(dict_feat) #appending the dict to the list\n",
    "\n",
    "    df = pd.DataFrame(list_features) #converting the list of dictionaries into a pandas dataframe, as seen at https://stackoverflow.com/questions/18837262/convert-python-dict-into-a-dataframe\n",
    "\n",
    "    #selecting the features that are needed for the model, which are now columns in the df\n",
    "    df = df[['embedding', \n",
    "         'pos_extracted', \n",
    "         'position_rel2pred',\n",
    "         'embedding_head',\n",
    "         'num_of_children',\n",
    "         'punct_extracted',\n",
    "         'head_pos',\n",
    "         'dep_path',\n",
    "         'cosine_similarity_w_predicate',\n",
    "        'pos_misc_feature',\n",
    "        'head_pp_feature',\n",
    "        'ner',\n",
    "        'propbank_arg']] \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2778855-4484-4f15-af48-3fae93f37889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gold_labels(data_file):\n",
    "    '''\n",
    "    Extract gold labels.\n",
    "    Return a list of gold labels.\n",
    "    \n",
    "    :param data_file: a list of objects, where each object represents one 'frame' in a sentence.\n",
    "    :type data_file: string\n",
    "    '''\n",
    "    labels = []\n",
    "    \n",
    "    for sentence in data_file:\n",
    "        for token_dict in sentence:\n",
    "            #adding gold label to labels\n",
    "            gold_label = token_dict['argument']\n",
    "            labels.append(gold_label)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bb76ab-d837-49d9-832b-83b0de190457",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file_path = 'data/en_ewt-up-dev.conllu'\n",
    "train_file_path = 'data/en_ewt-up-train.conllu'\n",
    "test_file_path = 'data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72f9117-10ff-4cf7-8071-3c1f8bef09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_data(dev_file_path)\n",
    "train_data = load_data(train_file_path)\n",
    "test_data = load_data(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ea8724-82cf-4380-b5ba-930af03b4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_for_model(train_data)\n",
    "dev_df = prepare_for_model(dev_data)\n",
    "test_df = prepare_for_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba64f7ce-59ea-4fcf-9368-594197c4323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding                            0\n",
       "pos_extracted                        0\n",
       "position_rel2pred                    0\n",
       "embedding_head                       0\n",
       "num_of_children                      0\n",
       "punct_extracted                      0\n",
       "head_pos                             0\n",
       "dep_path                             0\n",
       "cosine_similarity_w_predicate        0\n",
       "pos_misc_feature                     0\n",
       "head_pp_feature                  92870\n",
       "ner                                  0\n",
       "propbank_arg                     37528\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba87e6f-d7a3-42dd-a838-df6546af4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dict(dataframe):\n",
    "    \"\"\"Convert a pandas dataframe to a python dictionary.\n",
    "    Return a dictionary containing the name of the feature as key and the respective feature as value.\n",
    "\n",
    "    Parameter:\n",
    "    -dataframe: dataframe containing the features for the model.\n",
    "    \"\"\"\n",
    "    data_dict = dataframe.to_dict(orient='records')\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a552543a-19b9-4a5c-b525-8a75196603c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_dict = df_to_dict(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf61e60-630f-402f-8f29-dc06f4425305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'embedding': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'pos_extracted': 'PROPN', 'position_rel2pred': 'Before', 'embedding_head': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'num_of_children': 5, 'punct_extracted': 0, 'head_pos': 'root', 'dep_path': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'cosine_similarity_w_predicate': 0.0, 'pos_misc_feature': 'PROPN_no_space', 'head_pp_feature': 'PP-at', 'ner': 'PERSON', 'propbank_arg': '0_1_2'}, {'embedding': array([-5.2482e-01, -3.1963e-01, -1.1898e-01, -6.2672e-01,  4.3607e-02,\n",
      "        3.9176e-02, -7.4566e-01, -2.9516e-01, -7.0795e-01,  5.0644e-01,\n",
      "       -1.2069e-01,  6.0460e-01,  1.7881e-01, -3.2358e-01,  6.7840e-01,\n",
      "        6.1368e-01, -6.8220e-01, -9.0958e-01, -3.5056e-01, -5.0691e-01,\n",
      "        4.2474e-01,  3.3311e+00, -2.9048e-01,  3.1487e-01,  2.9800e-01,\n",
      "        3.0849e-01, -5.9682e-01, -9.0485e-02,  6.3417e-01,  1.1428e-01,\n",
      "        6.0949e-01,  4.2750e-01, -2.2331e-02,  8.1869e-01, -2.0090e-01,\n",
      "        4.0097e-01,  1.8467e-02, -3.2795e-01, -3.8703e-01,  8.9553e-01,\n",
      "        1.7345e-01, -2.0786e-01,  1.9661e-01,  3.3308e-01, -5.9757e-01,\n",
      "       -2.5713e-02,  5.4123e-01, -5.9299e-01,  4.3237e-02, -4.3826e-01,\n",
      "        1.3732e-01,  7.4199e-02,  2.0826e-01,  7.9075e-02,  3.6866e-01,\n",
      "       -1.0027e-01, -5.0150e-01, -9.9859e-02,  3.6711e-01,  4.9379e-01,\n",
      "        1.7505e-01,  9.4238e-02, -8.4836e-01, -4.9805e-02, -5.5745e-01,\n",
      "       -6.0496e-01, -1.3797e-01, -9.4929e-01,  1.0498e+00, -7.0823e-02,\n",
      "        4.5472e-01,  1.4697e-01,  9.8133e-02,  5.0213e-01,  4.6113e-01,\n",
      "        8.6423e-01, -1.0124e+00, -1.1020e+00,  1.9509e-01, -7.7223e-02,\n",
      "        6.2998e-01, -1.1331e+00, -3.6886e-01, -8.7241e-01,  5.5320e-03,\n",
      "        9.6174e-02,  9.6655e-01,  1.0610e-01,  5.6135e-01, -1.3818e+00,\n",
      "        8.0819e-01, -2.0846e-01,  4.1860e-01,  9.9999e-02, -6.6175e-01,\n",
      "        2.8933e-01, -8.9051e-01,  3.7495e-02,  4.6292e-02, -3.9863e-01,\n",
      "        2.4835e-01, -2.1226e-01,  6.4190e-01,  7.7302e-01,  1.2258e+00,\n",
      "       -7.2872e-01, -7.8351e-01,  1.6679e+00, -3.5259e-01, -4.0263e-01,\n",
      "        1.0265e-01, -2.4384e-01, -3.3026e-01,  5.3107e-03, -4.7305e-01,\n",
      "       -4.1587e-03,  2.9857e-01,  2.1387e-02, -6.5835e-01, -1.0130e-02,\n",
      "       -7.8507e-01,  7.4187e-03,  4.7464e-01,  8.6598e-01,  7.2612e-01,\n",
      "       -9.3103e-01,  2.8582e-01, -5.1885e-01,  8.5647e-01, -1.0641e-01,\n",
      "       -2.7125e-01, -2.9949e-01,  2.3144e-01, -3.7855e-01,  1.6950e-01,\n",
      "       -1.3784e-01,  3.6443e-01, -6.4500e-01, -3.2647e-01,  1.6657e-01,\n",
      "       -3.0835e-02, -4.9339e-01, -7.2254e-01,  1.0345e-01,  1.1010e+00,\n",
      "       -8.3535e-01,  7.5303e-02, -3.8513e-02, -1.1670e-02,  3.4800e-01,\n",
      "       -9.0702e-02,  7.9266e-02, -2.4702e-01, -1.4607e-01, -4.2316e-01,\n",
      "        4.9463e-02, -1.7431e-01,  4.1349e-01, -4.3356e-01, -6.5549e-01,\n",
      "       -3.1301e-01,  1.1720e-01, -1.4051e-01, -2.9881e-01,  2.0175e-01,\n",
      "       -5.1763e-01,  2.9630e-03,  5.2113e-01, -1.0565e-01,  1.1865e-01,\n",
      "        2.5795e-01, -2.7689e-01,  6.9176e-01, -3.2880e-02, -7.7013e-02,\n",
      "       -1.6738e-01, -1.3355e+00, -7.1185e-01, -7.6961e-01,  4.9021e-01,\n",
      "        6.6794e-01, -1.1177e-01,  9.0625e-02,  4.6741e-01,  8.8780e-01,\n",
      "       -1.3181e-01, -1.5339e+00,  1.1401e+00,  8.0685e-01, -8.0188e-02,\n",
      "       -8.7080e-01, -5.2954e-01,  4.1509e-01,  3.1474e-01, -2.3211e-01,\n",
      "        4.6550e-01,  3.4637e-01, -4.6424e-02,  5.8811e-01,  1.5755e+00],\n",
      "      dtype=float32), 'pos_extracted': 'PUNCT', 'position_rel2pred': 'Before', 'embedding_head': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'num_of_children': 0, 'punct_extracted': 1, 'head_pos': 'PROPN', 'dep_path': ['HYPH', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'cosine_similarity_w_predicate': 0.20821724832057953, 'pos_misc_feature': 'PUNCT_no_space', 'head_pp_feature': 'PP-in', 'ner': '_', 'propbank_arg': '0_1_2'}]\n"
     ]
    }
   ],
   "source": [
    "train_dicc = train_features_dict[:2]\n",
    "print(train_dicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3c2e0ba-efa5-44eb-a4f9-bf5848e5eed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-5.2482e-01 -3.1963e-01 -1.1898e-01 -6.2672e-01  4.3607e-02  3.9176e-02\n",
      " -7.4566e-01 -2.9516e-01 -7.0795e-01  5.0644e-01 -1.2069e-01  6.0460e-01\n",
      "  1.7881e-01 -3.2358e-01  6.7840e-01  6.1368e-01 -6.8220e-01 -9.0958e-01\n",
      " -3.5056e-01 -5.0691e-01  4.2474e-01  3.3311e+00 -2.9048e-01  3.1487e-01\n",
      "  2.9800e-01  3.0849e-01 -5.9682e-01 -9.0485e-02  6.3417e-01  1.1428e-01\n",
      "  6.0949e-01  4.2750e-01 -2.2331e-02  8.1869e-01 -2.0090e-01  4.0097e-01\n",
      "  1.8467e-02 -3.2795e-01 -3.8703e-01  8.9553e-01  1.7345e-01 -2.0786e-01\n",
      "  1.9661e-01  3.3308e-01 -5.9757e-01 -2.5713e-02  5.4123e-01 -5.9299e-01\n",
      "  4.3237e-02 -4.3826e-01  1.3732e-01  7.4199e-02  2.0826e-01  7.9075e-02\n",
      "  3.6866e-01 -1.0027e-01 -5.0150e-01 -9.9859e-02  3.6711e-01  4.9379e-01\n",
      "  1.7505e-01  9.4238e-02 -8.4836e-01 -4.9805e-02 -5.5745e-01 -6.0496e-01\n",
      " -1.3797e-01 -9.4929e-01  1.0498e+00 -7.0823e-02  4.5472e-01  1.4697e-01\n",
      "  9.8133e-02  5.0213e-01  4.6113e-01  8.6423e-01 -1.0124e+00 -1.1020e+00\n",
      "  1.9509e-01 -7.7223e-02  6.2998e-01 -1.1331e+00 -3.6886e-01 -8.7241e-01\n",
      "  5.5320e-03  9.6174e-02  9.6655e-01  1.0610e-01  5.6135e-01 -1.3818e+00\n",
      "  8.0819e-01 -2.0846e-01  4.1860e-01  9.9999e-02 -6.6175e-01  2.8933e-01\n",
      " -8.9051e-01  3.7495e-02  4.6292e-02 -3.9863e-01  2.4835e-01 -2.1226e-01\n",
      "  6.4190e-01  7.7302e-01  1.2258e+00 -7.2872e-01 -7.8351e-01  1.6679e+00\n",
      " -3.5259e-01 -4.0263e-01  1.0265e-01 -2.4384e-01 -3.3026e-01  5.3107e-03\n",
      " -4.7305e-01 -4.1587e-03  2.9857e-01  2.1387e-02 -6.5835e-01 -1.0130e-02\n",
      " -7.8507e-01  7.4187e-03  4.7464e-01  8.6598e-01  7.2612e-01 -9.3103e-01\n",
      "  2.8582e-01 -5.1885e-01  8.5647e-01 -1.0641e-01 -2.7125e-01 -2.9949e-01\n",
      "  2.3144e-01 -3.7855e-01  1.6950e-01 -1.3784e-01  3.6443e-01 -6.4500e-01\n",
      " -3.2647e-01  1.6657e-01 -3.0835e-02 -4.9339e-01 -7.2254e-01  1.0345e-01\n",
      "  1.1010e+00 -8.3535e-01  7.5303e-02 -3.8513e-02 -1.1670e-02  3.4800e-01\n",
      " -9.0702e-02  7.9266e-02 -2.4702e-01 -1.4607e-01 -4.2316e-01  4.9463e-02\n",
      " -1.7431e-01  4.1349e-01 -4.3356e-01 -6.5549e-01 -3.1301e-01  1.1720e-01\n",
      " -1.4051e-01 -2.9881e-01  2.0175e-01 -5.1763e-01  2.9630e-03  5.2113e-01\n",
      " -1.0565e-01  1.1865e-01  2.5795e-01 -2.7689e-01  6.9176e-01 -3.2880e-02\n",
      " -7.7013e-02 -1.6738e-01 -1.3355e+00 -7.1185e-01 -7.6961e-01  4.9021e-01\n",
      "  6.6794e-01 -1.1177e-01  9.0625e-02  4.6741e-01  8.8780e-01 -1.3181e-01\n",
      " -1.5339e+00  1.1401e+00  8.0685e-01 -8.0188e-02 -8.7080e-01 -5.2954e-01\n",
      "  4.1509e-01  3.1474e-01 -2.3211e-01  4.6550e-01  3.4637e-01 -4.6424e-02\n",
      "  5.8811e-01  1.5755e+00]\n"
     ]
    }
   ],
   "source": [
    "for token_dict in train_dicc:\n",
    "    lemma_vector = token_dict['embedding']\n",
    "    print(lemma_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3e5122-2c7e-4c07-a108-f8fb70cc72f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_values(row_dict, selected_features):\n",
    "    '''\n",
    "    Extract feature value pairs from row\n",
    "    \n",
    "    :param row: row from conllu file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_to_index = {'embedding': 0,\n",
    "                        'pos_extracted': 1, \n",
    "                        'position_rel2pred': 2, \n",
    "                        'embedding_head': 3,\n",
    "                        'num_of_children': 4, \n",
    "                        'punct_extracted': 5, \n",
    "                        'head_pos': 6, \n",
    "                        'dep_path': 7,\n",
    "                        'cosine_similarity_w_predicate': 8, \n",
    "                        'pos_misc_feature': 9,\n",
    "                        'head_pp_feature': 10,\n",
    "                        'ner': 11, \n",
    "                        'propbank_arg': 12}\n",
    "    \n",
    "    feature_values_dict = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values_dict[feature_name] = row_dict.get(feature_name)\n",
    "\n",
    "    return feature_values_dict\n",
    "\n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Create vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Take sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))  \n",
    "        combined_vectors.append(combined_vector)\n",
    "    \n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_features(df, vectorizer=None):\n",
    "    '''\n",
    "    Extract features and converts them into a vector.\n",
    "    \n",
    "    :param df: dataframe containing the features extracted.\n",
    "    :type df: pandas DataFrame.\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "\n",
    "    selected_features = [ \n",
    "                     'pos_extracted',\n",
    "                     #'position_rel2pred',\n",
    "                     #'head_pos',\n",
    "                     #'dep_path',\n",
    "                     #'pos_misc_feature',\n",
    "                     #'head_pp_feature',\n",
    "                     #'ner',\n",
    "                     #'propbank_arg'\n",
    "                    ] \n",
    "\n",
    "    features_dict_list = df_to_dict(df) #converting the df to dictionaries to extract the features and convert to vector representation\n",
    "    \n",
    "    for token_dict in features_dict_list:\n",
    "        lemma_vector = token_dict['embedding']\n",
    "        head_vector = token_dict['embedding_head']\n",
    "        cos_sim_vector = np.asarray([token_dict['cosine_similarity_w_predicate']]) #converting the numerical feat to np arrays to be concatenated in a single array\n",
    "        #num_children = np.asarray([token_dict['num_of_children']])\n",
    "        punct_extracted = np.asarray([token_dict['punct_extracted']])\n",
    "        dense_vectors.append(np.concatenate((lemma_vector,head_vector,cos_sim_vector,punct_extracted))) #contactenating embeddings plus numerical value features\n",
    "        #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "        #we thus only use other features with limited values\n",
    "        other_features = extract_feature_values(token_dict, selected_features)\n",
    "        traditional_features.append(other_features)\n",
    "\n",
    "    \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer\n",
    "    #return dense_vectors\n",
    "\n",
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Create classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier = LogisticRegression(max_iter=10000)\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier\n",
    "\n",
    "def label_data(vectorizer, testfile, classifier):\n",
    "    '''\n",
    "    Extract features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations = extract_features(testfile,vectorizer)\n",
    "    labels = extract_gold_labels(dev_data)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return labels,predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdcef27f-1e54-4a3c-b7a0-283df4367d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_confusion_matrix(GoldLabel, PredictLabel, label_set):\n",
    "    \"\"\"\n",
    "    use `sklearn.metric confusion_matrix` to create confusion matrix of model predict.\n",
    "    and `sklearn.metric ConfusionMatrixDisplay` to display created confusion matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    GoldLabel : list\n",
    "        list of all Gold labels\n",
    "    PredictLabel : list\n",
    "        list of all Prediction labels\n",
    "    label_set : list \n",
    "        list of all classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Confusion matrix\n",
    "    \"\"\"\n",
    "    cf_matrix = confusion_matrix(GoldLabel, PredictLabel) # create a confusion matrix with gold and predicts\n",
    "    print(cf_matrix) # print confusion_matrix as text\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, display_labels=label_set) # create graphical confusion_matrix\n",
    "    fig, ax = plt.subplots(figsize=(15,15)) # create bigger plot because there is many classes in this task\n",
    "    display.plot(ax =ax) # show confusion_matrix\n",
    "    plt.xticks(rotation=90) # rotate X label of plot 90 degree\n",
    "    plt.show() # show confusion matrix\n",
    "    return cf_matrix # return confusion_matrix (maybe useful later)\n",
    "\n",
    "def calculate_precision_recall_f1score(GoldLabel, PredictLabel, label_set): # function get gold and predict and set of labels\n",
    "    \"\"\"\n",
    "    use `sklearn.metric classification_report` to get report of model predict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    GoldLabel : list\n",
    "        list of all Gold labels\n",
    "    PredictLabel : list\n",
    "        list of all Prediction labels\n",
    "    label_set : list \n",
    "        list of all classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Classification report\n",
    "    \"\"\"\n",
    "    report = classification_report(GoldLabel, PredictLabel, digits = 3, target_names=label_set) # calculate report\n",
    "    print(report) # print report\n",
    "    return report # return report (maybe useful later)\n",
    "\n",
    "def evaluation_model(GoldLabel, PredictLabel): # get gold and predict\n",
    "    \"\"\"\n",
    "    Evaluation models by call `calculate_precision_recall_f1score` and `provide_confusion_matrix` functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data :\n",
    "        Train or test or dev dataset (after extracting fetures)\n",
    "    PredictLabel : list\n",
    "        list of all Prediction labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Classification report and Confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    label_set = sorted(set(GoldLabel)) # find uniqe lables in gold\n",
    "    print(label_set)\n",
    "\n",
    "    print('precision_recall_f1-score')\n",
    "    report = calculate_precision_recall_f1score(GoldLabel, PredictLabel, label_set) # calculate_precision_recall_f1score\n",
    "\n",
    "    print('Confusion matrix')\n",
    "    cf_matrix = provide_confusion_matrix(GoldLabel, PredictLabel, label_set) # provide_confusion_matrix\n",
    "\n",
    "    return report, cf_matrix # return report and cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "313aa079-531f-4ae3-977b-a722d7cf2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "245e4399-4106-4e9f-96e5-4ee7fb2d6b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding                          0\n",
       "pos_extracted                      0\n",
       "position_rel2pred                  0\n",
       "embedding_head                     0\n",
       "num_of_children                    0\n",
       "punct_extracted                    0\n",
       "head_pos                           0\n",
       "dep_path                           0\n",
       "cosine_similarity_w_predicate      0\n",
       "pos_misc_feature                   0\n",
       "head_pp_feature                  918\n",
       "ner                                0\n",
       "propbank_arg                     424\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67bc17bc-2d4d-4a35-b01b-c418e8c36632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier....\n",
      "Running evaluation...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m create_classifier(dense_feature_representations, gold[:\u001b[38;5;241m100\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning evaluation...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m gold_labels,predicted \u001b[38;5;241m=\u001b[39m label_data(vec,dev_df, classifier)\n\u001b[1;32m      8\u001b[0m evaluation_model(gold_labels,predicted)\n",
      "Cell \u001b[0;32mIn[16], line 154\u001b[0m, in \u001b[0;36mlabel_data\u001b[0;34m(vectorizer, testfile, classifier)\u001b[0m\n\u001b[1;32m    152\u001b[0m dense_feature_representations \u001b[38;5;241m=\u001b[39m extract_features(testfile,vectorizer)\n\u001b[1;32m    153\u001b[0m labels \u001b[38;5;241m=\u001b[39m extract_gold_labels(dev_data)\n\u001b[0;32m--> 154\u001b[0m predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(dense_feature_representations)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels,predictions\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_base.py:451\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    450\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 451\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    453\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_base.py:432\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    429\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    430\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 432\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "print('Extracting dense features...')\n",
    "dense_feature_representations,vec = extract_features(train_df)\n",
    "gold = extract_gold_labels(train_data)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, gold[:100])\n",
    "print('Running evaluation...')\n",
    "gold_labels,predicted = label_data(vec,dev_df, classifier)\n",
    "evaluation_model(gold_labels,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92795bdd-d744-42e2-b8e6-94d3effa221c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
